# ğŸš€ API Performance Tracer (.NET)

A lightweight, developer-focused tool that helps you understand **where your API time is being spent** â€” before production.

Think of it as a **â€œLighthouse for APIsâ€** â€” providing a simple breakdown of request latency across the backend pipeline.

---

## ğŸ¯ Project Goal

Give developers clear visibility into API performance by breaking down request time into meaningful stages, such as:

Total: 320ms
â”œâ”€ Middleware Overhead: 10ms
â”œâ”€ Controller Execution: 140ms
â””â”€ Response Pipeline: 170ms

yaml
Copy code

Instead of only seeing *â€œthis request took 320msâ€*, the tool helps you understand **why** it took that long and where bottlenecks exist.

---

## ğŸ§© Phase 1 â€” MVP Scope

### âœ… Step 1 â€” Request Timing Middleware (Core)

Implement an ASP.NET Core middleware that measures:

- total request time  
- middleware overhead  
- controller execution time  
- response pipeline time  

The middleware will output timing data in:

- human-readable console logs  
- machine-readable JSON trace format  

> This forms the **foundation of the tool**.

---

### â³ Step 2 â€” Database Timing (EF Core first)

Add optional instrumentation to capture database performance:

- total DB execution time  
- number of queries  
- per-query duration  
- slow query detection  

Future support:

- Dapper  
- ADO.NET  
- additional database providers via adapters  

---

### â³ Step 3 â€” Trace Export

Enable saving performance traces as JSON files so they can be:

- inspected later  
- compared between runs  
- consumed in CI benchmarking  
- visualized by external tools  

The trace format will loosely follow:

- OpenTelemetry concepts  
- Chrome Trace / waterfall timing style  

---

### â³ Step 4 â€” Simple Web UI Viewer (Future)

Provide a lightweight offline viewer that:

- visualizes traces as a **timeline / waterfall**  
- highlights bottlenecks  
- offers Lighthouse-style improvement hints  

The UI will be built **after the core features are stable**.

---

## ğŸ—ï¸ Planned Project Structure

api-perf/
â”œâ”€ ApiPerfDemo/ # Sample API using the tool
â”œâ”€ ApiPerf.Core/ # Library with middleware + tracers
â”œâ”€ ApiPerf.CLI/ # (future) CLI API test runner
â””â”€ README.md

yaml
Copy code

Development approach:

1. Build features inside `ApiPerfDemo`  
2. Extract reusable logic into `ApiPerf.Core` as a portable library  

---

## ğŸ’¡ Design Principles

- âš¡ Developer-first  
- ğŸ§© Zero-config setup  
- ğŸ› ï¸ Works locally & in CI  
- ğŸ§ª Focused on pre-production debugging  
- ğŸ‘ Open-source & extensible  
- ğŸ—ï¸ Lightweight â€” not a full APM tool  

This project is **not intended to replace** Datadog / New Relic / Grafana.  
Its purpose is to be:

> simple, fast, and useful during development.

---

## ğŸ Status

**Phase 1 â€” In Progress**

Current work:

- âœ”ï¸ Environment setup  
- â³ Step 1 â€” Request Timing Middleware  

Upcoming:

- database timing instrumentation  
- trace export support  

---

## ğŸ¤ Contributions & Roadmap

Areas planned for community contributions:

- framework integrations  
- database adapters  
- performance benchmarking ideas  
- documentation & examples  

A public roadmap and issue tracker will be added after the MVP is ready.

---

## ğŸ“š License

Planned license: **MIT**  
(Open, permissive, developer-friendly)

---

## ğŸ‘¥ Authors / Maintainers

Early-stage community project â€” contributors welcome.

---

## ğŸš§ Work in Progress

This repository is being built step-by-step with a focus on learning, collaboration, and improving backend performance visibility.

Follow along, experiment, test, and help make API optimization easier for everyone ğŸš€